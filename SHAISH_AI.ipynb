{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrcZd6Ihd5gP+rhm9+7eOo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShalinPatel670/ShalinPatel670/blob/main/SHAISH_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detects simple emotions from the facial expression images.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "train_dir = r\"C:\\Users\\25sha\\Downloads\\fer2013_jpg_format.zip\\train\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(48, 48),\n",
        "    batch_size=64,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(48, 48),\n",
        "    batch_size=64,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"validation\"\n",
        ")\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(7, activation='softmax')  # 7 emotion classes\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "model.save(\"emotion_model.h5\")\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JMo-JCp97910"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNqOxeFjx5DN"
      },
      "outputs": [],
      "source": [
        "# Detects which of the 8 simple emotions any voice is giving.\n",
        "\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
        "\n",
        "ravdess_path = r\"C:\\Users\\25sha\\Downloads\\archive\\audio_speech_actors_01-24\"\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "ravdess_directory_list = os.listdir(ravdess_path)\n",
        "for dir in ravdess_directory_list:\n",
        "    actor_folder = os.path.join(ravdess_path, dir)\n",
        "    if os.path.isdir(actor_folder):\n",
        "        for file in os.listdir(actor_folder):\n",
        "            part = file.split('.')[0].split('-')\n",
        "            if len(part) > 2:\n",
        "                emotion_label = int(part[2])\n",
        "                file_emotion.append(emotion_label)\n",
        "                file_path.append(os.path.join(actor_folder, file))\n",
        "\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "\n",
        "emotion_map = {\n",
        "    1: 'neutral', 2: 'calm', 3: 'happy', 4: 'sad',\n",
        "    5: 'angry', 6: 'fear', 7: 'disgust', 8: 'surprise'\n",
        "}\n",
        "ravdess_df.Emotions.replace(emotion_map, inplace=True)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(x=ravdess_df[\"Emotions\"])\n",
        "plt.xlabel(\"Emotion\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of Emotions in RAVDESS Dataset\")\n",
        "plt.show()\n",
        "\n",
        "def extract_features(file_path, max_length=500):\n",
        "    y, sr = librosa.load(file_path, sr=16000)\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "\n",
        "    if mfccs.shape[1] > max_length:\n",
        "        mfccs = mfccs[:, :max_length]\n",
        "    else:\n",
        "        pad_width = max_length - mfccs.shape[1]\n",
        "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "\n",
        "    return mfccs\n",
        "\n",
        "X = np.array([extract_features(f) for f in ravdess_df[\"Path\"]])\n",
        "X = X[..., np.newaxis]\n",
        "\n",
        "unique_emotions = list(emotion_map.values())\n",
        "y = np.array([unique_emotions.index(emotion) for emotion in ravdess_df[\"Emotions\"]])\n",
        "y = to_categorical(y, num_classes=len(unique_emotions))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential([\n",
        "    Conv1D(64, kernel_size=3, activation=\"relu\", input_shape=(40, 500, 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    BatchNormalization(),\n",
        "    Flatten(),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(unique_emotions), activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "model.save(\"/mnt/data/ravdess_emotion_model.h5\")\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encrypting facial expressions to avoid identification and ensure privacy.\n",
        "\n",
        "import os\n",
        "from Crypto.Cipher import AES\n",
        "from Crypto.Random import get_random_bytes\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "dataset_path = \"dataset\"\n",
        "encrypted_dataset_path = \"encrypted_dataset\"\n",
        "\n",
        "for split in [\"train\", \"test\"]:\n",
        "    split_path = os.path.join(encrypted_dataset_path, split)\n",
        "    if not os.path.exists(split_path):\n",
        "        os.makedirs(split_path)\n",
        "\n",
        "key_path = \"encryption_key.bin\"\n",
        "\n",
        "if not os.path.exists(key_path):\n",
        "    encryption_key = get_random_bytes(32)\n",
        "    with open(key_path, \"wb\") as key_file:\n",
        "        key_file.write(encryption_key)\n",
        "else:\n",
        "    with open(key_path, \"rb\") as key_file:\n",
        "        encryption_key = key_file.read()\n",
        "\n",
        "def encrypt_image(image_path, key):\n",
        "    image = cv2.imread(image_path)\n",
        "    _, img_encoded = cv2.imencode('.png', image)\n",
        "    image_bytes = img_encoded.tobytes()\n",
        "\n",
        "    cipher = AES.new(key, AES.MODE_EAX)\n",
        "    ciphertext, tag = cipher.encrypt_and_digest(image_bytes)\n",
        "\n",
        "    return cipher.nonce + ciphertext\n",
        "\n",
        "for split in [\"train\", \"test\"]:\n",
        "    for emotion_folder in os.listdir(os.path.join(dataset_path, split)):\n",
        "        folder_path = os.path.join(dataset_path, split, emotion_folder)\n",
        "        encrypted_folder_path = os.path.join(encrypted_dataset_path, split, emotion_folder)\n",
        "\n",
        "        if not os.path.exists(encrypted_folder_path):\n",
        "            os.makedirs(encrypted_folder_path)\n",
        "\n",
        "        for image_file in os.listdir(folder_path):\n",
        "            image_path = os.path.join(folder_path, image_file)\n",
        "            encrypted_data = encrypt_image(image_path, encryption_key)\n",
        "\n",
        "            encrypted_image_path = os.path.join(encrypted_folder_path, image_file + \".enc\")\n",
        "            with open(encrypted_image_path, \"wb\") as enc_file:\n",
        "                enc_file.write(encrypted_data)\n",
        "\n",
        "print(\"Dataset Encryption Complete! All images are encrypted.\")"
      ],
      "metadata": {
        "id": "L0IEy-3_34Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decrypting so only those with the keys can even open it-- but in any case,\n",
        "# the file will just go straight for use in the model.\n",
        "\n",
        "import os\n",
        "from Crypto.Cipher import AES\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "encrypted_dataset_path = \"encrypted_dataset\"\n",
        "decrypted_dataset_path = \"decrypted_dataset\"\n",
        "key_path = \"encryption_key.bin\"\n",
        "\n",
        "for split in [\"train\", \"test\"]:\n",
        "    split_path = os.path.join(decrypted_dataset_path, split)\n",
        "    if not os.path.exists(split_path):\n",
        "        os.makedirs(split_path)\n",
        "\n",
        "with open(key_path, \"rb\") as key_file:\n",
        "    encryption_key = key_file.read()\n",
        "\n",
        "def decrypt_image(encrypted_image_path, key):\n",
        "    with open(encrypted_image_path, \"rb\") as file:\n",
        "        encrypted_data = file.read()\n",
        "\n",
        "    nonce = encrypted_data[:16]\n",
        "    ciphertext = encrypted_data[16:]\n",
        "\n",
        "    cipher = AES.new(key, AES.MODE_EAX, nonce=nonce)\n",
        "    decrypted_data = cipher.decrypt(ciphertext)\n",
        "\n",
        "    np_arr = np.frombuffer(decrypted_data, np.uint8)\n",
        "    img = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
        "\n",
        "    return img\n",
        "\n",
        "for split in [\"train\", \"test\"]:\n",
        "    for emotion_folder in os.listdir(os.path.join(encrypted_dataset_path, split)):\n",
        "        folder_path = os.path.join(encrypted_dataset_path, split, emotion_folder)\n",
        "        decrypted_folder_path = os.path.join(decrypted_dataset_path, split, emotion_folder)\n",
        "\n",
        "        if not os.path.exists(decrypted_folder_path):\n",
        "            os.makedirs(decrypted_folder_path)\n",
        "\n",
        "        for image_file in os.listdir(folder_path):\n",
        "            encrypted_image_path = os.path.join(folder_path, image_file)\n",
        "            decrypted_img = decrypt_image(encrypted_image_path, encryption_key)\n",
        "            decrypted_image_path = os.path.join(decrypted_folder_path, image_file.replace(\".enc\", \".png\"))\n",
        "            cv2.imwrite(decrypted_image_path, decrypted_img)\n",
        "\n",
        "print(\"Decryption Complete! Images restored.\")"
      ],
      "metadata": {
        "id": "ZHJF9kzH3veA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Circumplex flood. Every emotion is mapped using the circumplex model,\n",
        "# and can be vectorized with (valence, arousal) [valence is positive/negative, arousal is low-energy/high-energy.]\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# mapping the emotions using the circumplex model...\n",
        "emotion_mapping = {\n",
        "    \"angry\": (-0.7, 0.7),\n",
        "    \"disgust\": (-0.7, 0.5),\n",
        "    \"fear\": (-0.6, 0.8),\n",
        "    \"happy\": (0.7, 0.7),\n",
        "    \"neutral\": (0.0, 0.0),\n",
        "    \"sad\": (-0.8, -0.6),\n",
        "    \"surprise\": (0.6, 0.8)\n",
        "}\n",
        "\n",
        "complex_emotion_mapping = {\n",
        "    \"Distressed\": (-0.4, 0.6),\n",
        "    \"Tense\": (-0.3, 0.4),\n",
        "    \"Alert\": (0.1, 0.5),\n",
        "    \"Excited\": (0.5, 0.6),\n",
        "    \"Content\": (0.4, 0.3),\n",
        "    \"Depressed\": (-0.5, -0.5),\n",
        "    \"Bored\": (-0.2, -0.4),\n",
        "    \"Neutral\": (0.0, 0.0)\n",
        "}\n",
        "\n",
        "simple_emotions = list(emotion_mapping.keys())\n",
        "complex_emotions = list(complex_emotion_mapping.keys())\n",
        "\n",
        "# Using synthetic simple emotions, using arclength on circumplex model to derive complex emotions.\n",
        "# Less arclength between simple and complex emotion = stronger weight, as the two are more intertwined.\n",
        "# This simulates how simple emotions form complex ones in a natural way.\n",
        "\n",
        "def compute_weights():\n",
        "    weights = np.zeros((len(simple_emotions), len(complex_emotions)))\n",
        "    for i, simple in enumerate(simple_emotions):\n",
        "        for j, complex_ in enumerate(complex_emotions):\n",
        "            v1, a1 = emotion_mapping[simple]\n",
        "            v2, a2 = complex_emotion_mapping[complex_]\n",
        "            arclength = np.arccos((v1 * v2 + a1 * a2) / (np.sqrt(v1**2 + a1**2) * np.sqrt(v2**2 + a2**2) + 1e-6))\n",
        "            weights[i, j] = 1 / (arclength + 0.1)\n",
        "    weights /= weights.sum(axis=1, keepdims=True)\n",
        "    return weights\n",
        "\n",
        "weights_matrix = compute_weights()\n",
        "\n",
        "num_samples = 200\n",
        "X_synthetic = np.random.dirichlet(np.ones(len(simple_emotions)) * 3, size=num_samples)\n",
        "y_synthetic_probs = np.dot(X_synthetic, weights_matrix)\n",
        "y_synthetic_labels = np.argmax(y_synthetic_probs, axis=1)\n",
        "y_synthetic_emotions = [complex_emotions[label] for label in y_synthetic_labels]\n",
        "\n",
        "df_synthetic = pd.DataFrame(X_synthetic, columns=simple_emotions)\n",
        "df_synthetic[\"Complex Emotion\"] = y_synthetic_emotions\n",
        "y_synthetic_encoded = np.array([complex_emotions.index(emotion) for emotion in y_synthetic_emotions])\n",
        "\n",
        "# deep learning uses the synthetic dataset involving the simple and derived complex emotions.\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation='relu', input_shape=(len(simple_emotions),)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(complex_emotions), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_synthetic, y_synthetic_encoded, epochs=50, batch_size=16, verbose=1)\n",
        "\n",
        "test_samples = 50\n",
        "X_test = np.random.dirichlet(np.ones(len(simple_emotions)) * 3, size=test_samples)\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
        "predicted_complex_emotions = [complex_emotions[label] for label in y_pred_labels]\n",
        "\n",
        "# plotting data.\n",
        "\n",
        "df_predictions = pd.DataFrame(X_test, columns=simple_emotions)\n",
        "df_predictions[\"Predicted Complex Emotion\"] = predicted_complex_emotions\n",
        "\n",
        "counts = pd.Series(predicted_complex_emotions).value_counts().reindex(complex_emotions, fill_value=0)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "counts.plot(kind=\"bar\", color=\"purple\", edgecolor=\"black\")\n",
        "plt.xlabel(\"Predicted Complex Emotion\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Predicted Complex Emotions\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "hZ0aX20Ix67v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Used botpress to make an LLM trained on research and other info on crisis situations and therapy. It\n",
        "\n",
        "<script src=\"https://cdn.botpress.cloud/webchat/v1/inject.js\"></script>\n",
        "<script>\n",
        "  window.botpressWebChat.init({\n",
        "      \"composerPlaceholder\": \"Chat with your bot\",\n",
        "      \"botConversationDescription\": \"I'm here to assist you!\",\n",
        "      \"botId\": \"7416bbf4-4223-4490-9169-01c5a3f2ae7a\",\n",
        "      \"hostUrl\": \"https://cdn.botpress.cloud/webchat/v1\",\n",
        "      \"messagingUrl\": \"https://messaging.botpress.cloud/\",\n",
        "      \"clientId\": \"7416bbf4-4223-4490-9169-01c5a3f2ae7a\",\n",
        "      \"webhookId\": \"8baf76d9-c0e8-4718-8005-b23c093b8543\",\n",
        "      \"lazySocket\": true,\n",
        "      \"themeName\": \"prism\",\n",
        "      \"botName\": \"YourBot\",\n",
        "      \"frontendVersion\": \"v1\",\n",
        "      \"useSessionStorage\": true,\n",
        "      \"theme\": \"prism\",\n",
        "      \"themeColor\": \"#2563eb\",\n",
        "      \"allowedOrigins\": []\n",
        "  });\n",
        "</script>\n",
        "<script src=\"https://mediafiles.botpress.cloud/7416bbf4-4223-4490-9169-01c5a3f2ae7a/webchat/config.js\" defer></script>"
      ],
      "metadata": {
        "id": "blgW_sVTBnqe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}